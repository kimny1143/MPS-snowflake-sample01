-- Database and Schema setup
CREATE DATABASE IF NOT EXISTS MUED;
USE DATABASE MUED;

CREATE SCHEMA IF NOT EXISTS RAW;
CREATE SCHEMA IF NOT EXISTS STG;
CREATE SCHEMA IF NOT EXISTS CORE;

-- File format for JSON
CREATE OR REPLACE FILE FORMAT RAW.JSON_FORMAT
    TYPE = 'JSON'
    COMPRESSION = 'AUTO'
    STRIP_OUTER_ARRAY = TRUE;

-- Internal stage for RSS data
CREATE OR REPLACE STAGE RAW.RSS_STAGE
    FILE_FORMAT = RAW.JSON_FORMAT;

-- RAW layer: Store raw RSS data
CREATE OR REPLACE TABLE RAW.NOTE_RSS_RAW (
    ID VARCHAR(36) DEFAULT UUID_STRING(),
    SOURCE_URL VARCHAR(500),
    FETCHED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    RAW_DATA VARIANT,
    PRIMARY KEY (ID)
);

-- STG layer: Parse and normalize RSS data
CREATE OR REPLACE VIEW STG.NOTE_ARTICLES AS
SELECT
    r.ID as RAW_ID,
    r.SOURCE_URL,
    r.FETCHED_AT,
    f.value:id::VARCHAR as ARTICLE_ID,
    f.value:title::VARCHAR as TITLE,
    f.value:url::VARCHAR as URL,
    TRY_TO_TIMESTAMP_NTZ(f.value:published_at::VARCHAR) as PUBLISHED_AT,
    f.value:body::VARCHAR as BODY,
    LENGTH(f.value:body::VARCHAR) as BODY_LENGTH
FROM RAW.NOTE_RSS_RAW r,
LATERAL FLATTEN(input => r.RAW_DATA) f;

-- CORE layer: Business-ready articles with deduplication
CREATE OR REPLACE TABLE CORE.BLOG_POSTS (
    ID VARCHAR(36) PRIMARY KEY,
    TITLE VARCHAR(500),
    URL VARCHAR(500) UNIQUE,
    PUBLISHED_AT TIMESTAMP_NTZ,
    BODY TEXT,
    BODY_LENGTH INTEGER,
    FIRST_FETCHED_AT TIMESTAMP_NTZ,
    LAST_UPDATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- Stream to capture changes in STG layer
CREATE OR REPLACE STREAM STG.NOTE_ARTICLES_STREAM ON VIEW STG.NOTE_ARTICLES
    APPEND_ONLY = TRUE;

-- Merge procedure for CORE layer
CREATE OR REPLACE PROCEDURE CORE.MERGE_BLOG_POSTS()
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
BEGIN
    MERGE INTO CORE.BLOG_POSTS t
    USING (
        SELECT
            ARTICLE_ID,
            TITLE,
            URL,
            PUBLISHED_AT,
            BODY,
            BODY_LENGTH,
            MIN(FETCHED_AT) as FIRST_FETCHED_AT
        FROM STG.NOTE_ARTICLES_STREAM
        WHERE METADATA$ACTION = 'INSERT'
        GROUP BY ARTICLE_ID, TITLE, URL, PUBLISHED_AT, BODY, BODY_LENGTH
    ) s
    ON t.URL = s.URL
    WHEN MATCHED AND (t.TITLE != s.TITLE OR t.BODY != s.BODY) THEN
        UPDATE SET
            TITLE = s.TITLE,
            BODY = s.BODY,
            BODY_LENGTH = s.BODY_LENGTH,
            LAST_UPDATED_AT = CURRENT_TIMESTAMP()
    WHEN NOT MATCHED THEN
        INSERT (ID, TITLE, URL, PUBLISHED_AT, BODY, BODY_LENGTH, FIRST_FETCHED_AT)
        VALUES (s.ARTICLE_ID, s.TITLE, s.URL, s.PUBLISHED_AT, s.BODY, s.BODY_LENGTH, s.FIRST_FETCHED_AT);

    RETURN 'Merge completed: ' || SQLROWCOUNT || ' rows affected';
END;
$$;

-- Task to run merge every 5 minutes
CREATE OR REPLACE TASK CORE.MERGE_BLOG_POSTS_TASK
    WAREHOUSE = COMPUTE_WH
    SCHEDULE = '5 MINUTE'
    WHEN SYSTEM$STREAM_HAS_DATA('STG.NOTE_ARTICLES_STREAM')
AS
    CALL CORE.MERGE_BLOG_POSTS();

-- Grant necessary permissions
GRANT USAGE ON DATABASE MUED TO ROLE ACCOUNTADMIN;
GRANT ALL ON SCHEMA RAW TO ROLE ACCOUNTADMIN;
GRANT ALL ON SCHEMA STG TO ROLE ACCOUNTADMIN;
GRANT ALL ON SCHEMA CORE TO ROLE ACCOUNTADMIN;

-- Enable the task (must be done separately after creation)
-- ALTER TASK CORE.MERGE_BLOG_POSTS_TASK RESUME;
